{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Perception Pick & Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Steps for a Passing Submission:\n",
    "1. Extract features and train an SVM model on new objects (see `pick_list_*.yaml` in `/pr2_robot/config/` for the list of models you'll be trying to identify). \n",
    "2. Write a ROS node and subscribe to `/pr2/world/points` topic. This topic contains noisy point cloud data that you must work with.\n",
    "3. Use filtering and RANSAC plane fitting to isolate the objects of interest from the rest of the scene.\n",
    "4. Apply Euclidean clustering to create separate clusters for individual items.\n",
    "5. Perform object recognition on these objects and assign them labels (markers in RViz).\n",
    "6. Calculate the centroid (average in x, y and z) of the set of points belonging to that each object.\n",
    "7. Create ROS messages containing the details of each object (name, pick_pose, etc.) and write these messages out to `.yaml` files, one for each of the 3 scenarios (`test1-3.world` in `/pr2_robot/worlds/`).  [See the example `output.yaml` for details on what the output should look like.](https://github.com/udacity/RoboND-Perception-Project/blob/master/pr2_robot/config/output.yaml)  \n",
    "8. Submit a link to your GitHub repo for the project or the Python code for your perception pipeline and your output `.yaml` files (3 `.yaml` files, one for each test world).  You must have correctly identified 100% of objects from `pick_list_1.yaml` for `test1.world`, 80% of items from `pick_list_2.yaml` for `test2.world` and 75% of items from `pick_list_3.yaml` in `test3.world`.\n",
    "9. Congratulations!  Your Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rubric Points](https://review.udacity.com/#!/rubrics/1067/view) \n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  \n",
    "\n",
    "I wrote it in the format of a Jupyter Notebook as usual.\n",
    "\n",
    "### Exercise 1, 2 and 3 pipeline implemented\n",
    "#### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is part of the code. The complete implemenation is gathered into a model called RANSAC.py. The details of the tweaking for the parameters is noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Point Cloud file\n",
    "cloud = pcl.load_XYZRGB('tabletop.pcd')\n",
    "\n",
    "# Voxel Grid filter\n",
    "# Create a VoxelGrid filter object for out input point cloud\n",
    "vox = cloud.make_voxel_grid_filter()\n",
    "\n",
    "# choose a voxel (leaf) size\n",
    "# LEAF_SIZE = 0.0001 # this is warned by the script to be too small\n",
    "# LEAF_SIZE = 0.001 # same as above\n",
    "LEAF_SIZE = 0.01  # no warning for this value, try a larger value\n",
    "# LEAF_SIZE = 0.1 # this is too large to show any feature\n",
    "# tried 0.01 ~ 0.05, 0.01 is the best\n",
    "\n",
    "# Set voxel size\n",
    "vox.set_leaf_size(LEAF_SIZE, LEAF_SIZE, LEAF_SIZE)\n",
    "\n",
    "# call the filter funciton to obtain the resultant downsampled point cloud\n",
    "cloud_filtered = vox.filter()\n",
    "filename = 'voxel_downsampled.pcd'\n",
    "pcl.save(cloud_filtered, filename)\n",
    "\n",
    "# PassThrough filter\n",
    "# Create a PassThrough filter objects\n",
    "passthrough = cloud_filtered.make_passthrough_filter()\n",
    "\n",
    "# Assign axis and range to the passthrough filter objects\n",
    "filter_axis = 'z'\n",
    "passthrough.set_filter_field_name(filter_axis)\n",
    "\n",
    "# set the limits\n",
    "# for the quiz in lesson 3-11\n",
    "#axis_min = 0.8  # this removes the table\n",
    "#axis_max = 2\n",
    "\n",
    "#axis_min = 0.1  # this removes the objects on the table, leaves only the table\n",
    "#axis_max = 0.8\n",
    "\n",
    "#axis_min = 0  # this includes everything\n",
    "#axis_max = 2\n",
    "\n",
    "axis_min = 0.6  # this retains the table and the objects\n",
    "axis_max = 1.1\n",
    "\n",
    "passthrough.set_filter_limits(axis_min, axis_max)\n",
    "\n",
    "# Finally, use the filter function to obtain the resultant point cloud\n",
    "cloud_filtered = passthrough.filter()\n",
    "filename = 'pass_through_filtered.pcd'\n",
    "pcl.save(cloud_filtered, filename)\n",
    "\n",
    "# RANSAC plane segmentation\n",
    "# Create the segmentation object\n",
    "seg = cloud_filtered.make_segmenter()\n",
    "\n",
    "# Set the model you wish to filter\n",
    "seg.set_model_type(pcl.SACMODEL_PLANE)\n",
    "seg.set_method_type(pcl.SAC_RANSAC)\n",
    "\n",
    "# Max distance for a point to be considered fitting the model\n",
    "# For lesson 3-15 quiz 1\n",
    "# max_distance = 1  # this includes everything\n",
    "# max_distance = 0.1 # this removes the tall objects\n",
    "max_distance = 0.01 # this leaves only the table\n",
    "# max_distance = 0.001 # this might be unnessessarily high\n",
    "\n",
    "seg.set_distance_threshold(max_distance)\n",
    "\n",
    "# Call the segment function to obtain set of inlier indices and model coefficients\n",
    "inliers, coefficients = seg.segment()\n",
    "\n",
    "# Extract inliers\n",
    "extracted_inliers = cloud_filtered.extract(inliers, negative=False)\n",
    "filename = 'extracted_inliers.pcd'\n",
    "pcl.save(extracted_inliers, filename)\n",
    "\n",
    "# Extract outliers\n",
    "extracted_outliers = cloud_filtered.extract(inliers, negative=True)\n",
    "filename = 'extracted_outliers.pcd'\n",
    "pcl.save(extracted_outliers, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is part of the code. The complete implemenation is gathered into a model called segmentation.py. The details of the tweaking for the parameters is noted in the comments. All the filtering functions are imported through the RANSAC module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pcl_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b34674ed3746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpcl_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mRANSAC\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define functions as required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pcl_helper'"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from pcl_helper import *\n",
    "from RANSAC import *\n",
    "\n",
    "# Define functions as required\n",
    "def clustering(pcl_data):\n",
    "    # Create k-d tree\n",
    "    white_cloud = XYZRGB_to_XYZ(pcl_data)\n",
    "    tree = white_cloud.make_kdtree()\n",
    "\n",
    "    # Create a cluster extraction object\n",
    "    ec = white_cloud.make_EuclideanClusterExtraction()\n",
    "\n",
    "    # Set tolerances for distance threshold\n",
    "    # 0.001: too small, the cluster will be too fractrized, colored like rainbow\n",
    "    # 0.01: less colorful, but still mixing multiple colors onto one object\n",
    "    # 0.1: nothing will show up except for the bowl at the center\n",
    "    # this means the min and max numbers need to be tweaked, more specifically,\n",
    "    # increase maximum\n",
    "    # when it was increased to 5000, all the objects are shown, now can get back\n",
    "    # to tweak this number between 0.01 and 0.1\n",
    "    # a unique color is achived for each object when this number = 0.014\n",
    "    ec.set_ClusterTolerance(0.014)\n",
    "\n",
    "    # Set minimum cluster size\n",
    "    # the lower this number, the more points each cluster has\n",
    "    # but it cannot be too small otherwise there will be \"noises\" inside each\n",
    "    # object\n",
    "    ec.set_MinClusterSize(5)\n",
    "\n",
    "    # Set maximum cluster size\n",
    "    # the larger this number, the more ponints will show up\n",
    "    # but it cannot be too large otherwise two objects may be colored into one\n",
    "    ec.set_MaxClusterSize(10000)\n",
    "\n",
    "    # Search the k-d tree for clusters\n",
    "    ec.set_SearchMethod(tree)\n",
    "\n",
    "    # Extract indices for each of the discovered clusters\n",
    "    cluster_indices = ec.Extract()\n",
    "    return white_cloud, tree, cluster_indices\n",
    "\n",
    "def color_clusters(pcl_data_xyz, cluster_indices):\n",
    "    # Assign a color corresponding to each segmented object in scene\n",
    "    cluster_color = get_color_list(len(cluster_indices))\n",
    "\n",
    "    # create a list for the colored cluster points\n",
    "    color_cluster_point_list = []\n",
    "\n",
    "    # traverse the indices and append to the list\n",
    "    for j, indices in enumerate(cluster_indices):\n",
    "        for i, indice in enumerate(indices):\n",
    "            color_cluster_point_list.append([pcl_data_xyz[indice][0],\n",
    "                                             pcl_data_xyz[indice][1],\n",
    "                                             pcl_data_xyz[indice][2],\n",
    "                                             rgb_to_float(cluster_color[j])])\n",
    "    # Create new cloud containing all clusters colored\n",
    "    cluster_cloud_colored = pcl.PointCloud_PointXYZRGB()\n",
    "    cluster_cloud_colored.from_list(color_cluster_point_list)\n",
    "    return cluster_cloud_colored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercise, I was using 50 loops, HSV mode instead of RGB, and linear kernel. This was later changed to rbf kernel for the project. \n",
    "\n",
    "A major obstacle I encountered during the implementation of Exercise 3 is that my svm was doing poorly even after I followed all the suggestions in Lesson 5-16. As I mentioned above, I have modified the histograms functions, increased the loop number, changed the filter and even tried to change the svm kernel. All failed. Now think of it, based on the fact that there's basically no change on the performance of my svm, I should have come to the conclusion way earlier than I had, that it must be something else than wrong code that's causing this. Sadly I spent way too many days just trying to tweak all the parameters to the extreme and hoping it will work magically.\n",
    "\n",
    "It turns out, that I was editing the wrong capture_features.py file inside the Exercise-3 folder instead of the one inside catkin_ws all the time. And I was so over my head when I discovered this that I spent another week tweaking all parameters without stopping to check if I have missed anything else. I made no obvious improvement during that week.\n",
    "\n",
    "After talking to the third expert without a result, I suddenly realized, like a flash, that I only fixed the capture_features.py but not other files with the same problem. There was a features.py file that also requires modification, and I also made the changes inside the wrong folder.\n",
    "\n",
    "From then it was all good. I changed the files in the correct folder, and my svm works like a charm. I can get a 70%~90% recognition with only 50 loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the computation for the color and normal histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_color_histograms(cloud, using_hsv=False):\n",
    "\n",
    "    # Compute histograms for the clusters\n",
    "    point_colors_list = []\n",
    "\n",
    "    # Step through each point in the point cloud\n",
    "    for point in pc2.read_points(cloud, skip_nans=True):\n",
    "        rgb_list = float_to_rgb(point[3])\n",
    "        if using_hsv:\n",
    "            point_colors_list.append(rgb_to_hsv(rgb_list) * 255)\n",
    "        else:\n",
    "            point_colors_list.append(rgb_list)\n",
    "\n",
    "    # Populate lists with color values\n",
    "    channel_1_vals = []\n",
    "    channel_2_vals = []\n",
    "    channel_3_vals = []\n",
    "\n",
    "    for color in point_colors_list:\n",
    "        channel_1_vals.append(color[0])\n",
    "        channel_2_vals.append(color[1])\n",
    "        channel_3_vals.append(color[2])\n",
    "\n",
    "    # Compute histograms\n",
    "    channel_1_hist = np.histogram(channel_1_vals, bins=32, range=(0, 256))\n",
    "    channel_2_hist = np.histogram(channel_2_vals, bins=32, range=(0, 256))\n",
    "    channel_3_hist = np.histogram(channel_3_vals, bins=32, range=(0, 256))\n",
    "\n",
    "    # Concatenate and normalize the histograms\n",
    "    hist_features = np.concatenate((channel_1_hist[0], channel_2_hist[0], channel_3_hist[0])).astype(np.float64)\n",
    "\n",
    "    normed_features = hist_features / np.sum(hist_features)\n",
    "    return normed_features\n",
    "\n",
    "\n",
    "def compute_normal_histograms(normal_cloud):\n",
    "    norm_x_vals = []\n",
    "    norm_y_vals = []\n",
    "    norm_z_vals = []\n",
    "\n",
    "    for norm_component in pc2.read_points(normal_cloud,\n",
    "                                          field_names = ('normal_x', 'normal_y', 'normal_z'),\n",
    "                                          skip_nans=True):\n",
    "        norm_x_vals.append(norm_component[0])\n",
    "        norm_y_vals.append(norm_component[1])\n",
    "        norm_z_vals.append(norm_component[2])\n",
    "\n",
    "    # Compute histograms of normal values (just like with color)\n",
    "    x_hist = np.histogram(norm_x_vals, bins=32, range=(-1, 1))\n",
    "    y_hist = np.histogram(norm_y_vals, bins=32, range=(-1, 1))\n",
    "    z_hist = np.histogram(norm_z_vals, bins=32, range=(-1, 1))\n",
    "\n",
    "    # Concatenate and normalize the histograms\n",
    "    hist_features = np.concatenate((x_hist[0], y_hist[0], z_hist[0])).astype(np.float64)\n",
    "\n",
    "    normed_features = hist_features / np.sum(hist_features)\n",
    "    return normed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the major part of the capture_features module. As you can see I am using 50 loops and HSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for model_name in models:\n",
    "        spawn_model(model_name)\n",
    "\n",
    "        for i in range(50):\n",
    "            # make five attempts to get a valid a point cloud then give up\n",
    "            sample_was_good = False\n",
    "            try_count = 0\n",
    "            while not sample_was_good and try_count < 5:\n",
    "                sample_cloud = capture_sample()\n",
    "                sample_cloud_arr = ros_to_pcl(sample_cloud).to_array()\n",
    "\n",
    "                # Check for invalid clouds.\n",
    "                if sample_cloud_arr.shape[0] == 0:\n",
    "                    print('Invalid cloud detected')\n",
    "                    try_count += 1\n",
    "                else:\n",
    "                    sample_was_good = True\n",
    "\n",
    "            # Extract histogram features\n",
    "            chists = compute_color_histograms(sample_cloud, using_hsv=True)\n",
    "            normals = get_normals(sample_cloud)\n",
    "            nhists = compute_normal_histograms(normals)\n",
    "            feature = np.concatenate((chists, nhists))\n",
    "            labeled_features.append([feature, model_name])\n",
    "\n",
    "        delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the confusion matrix of my svm for Exercise 3.\n",
    "![demo-1](https://user-images.githubusercontent.com/20687560/28748231-46b5b912-7467-11e7-8778-3095172b7b19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and Place Setup\n",
    "\n",
    "#### 1. For all three tabletop setups (`test*.world`), perform object recognition, then read in respective pick list (`pick_list_*.yaml`). Next construct the messages that would comprise a valid `PickPlace` request output them to `.yaml` format.\n",
    "\n",
    "And here's another image! \n",
    "![demo-2](https://user-images.githubusercontent.com/20687560/28748286-9f65680e-7468-11e7-83dc-f1a32380b89c.png)\n",
    "\n",
    "Spend some time at the end to discuss your code, what techniques you used, what worked and why, where the implementation might fail and how you might improve it if you were going to pursue this project further.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the svm needs to be trained for the test world. In my modified capture_features module, I have listed the models for each world in its correspoding pick list file, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the confusion matrix for the models for test world 1. The model file was named model_1.sav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for the models for test world 2. The model file was named model_2.sav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for the models for test world 3. The model file was named model_3.sav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the project, I have organized the perception pipeline inside the perception.py module. As you can see from below, the callback function is mostly performing segmentation as in Exercise 3, and the major part of the ROS message construction and yaml file output is done inside the mover function. Since I am skipping the actual pick and place movement for now, I have commented out the related code to speed things up.\n",
    "\n",
    "Another optimization I have done is that, instead of traversing the detected object list for comparison for each object in the pick list, I constructed a dictionary using the label of the detected object as the key and its centroid as the value. That way we can save a double loop and gain access to the centroid quickly later.\n",
    "\n",
    "The output file is named output_1.yaml, output_2.yaml, output_3.yaml as required and is included inside this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to load parameters and request PickPlace service\n",
    "def pr2_mover(object_list):\n",
    "\n",
    "    # Initialize variables\n",
    "    test_scene_num = Int32()\n",
    "    test_scene_num.data = 1  # test world 1\n",
    "    #test_scene_num.data = 2  # test world 2\n",
    "    #test_scene_num.data = 3  # test world 3\n",
    "\n",
    "    object_name = String()\n",
    "    arm_name = String()\n",
    "    pick_pose = Pose()\n",
    "    place_pose = Pose()\n",
    "\n",
    "    # create a dictionary to hold the names and its cloud data in the object_list\n",
    "    centroids_dic = {}\n",
    "\n",
    "    # traverse the object_list, build the dictionary\n",
    "    for detected_object in object_list:\n",
    "        label = detected_object.label\n",
    "        points_arr = ros_to_pcl(detected_object.cloud).to_array()\n",
    "        centroid = np.mean(points_arr, axis=0)[:3]\n",
    "        centroids_dic[label] = centroid\n",
    "\n",
    "    # Get/Read parameters\n",
    "    object_list_param = rospy.get_param('/object_list')\n",
    "    drop_pose_param = rospy.get_param('/dropbox')\n",
    "\n",
    "    for i in range(0, len(drop_pose_param)):\n",
    "        if drop_pose_param[i]['name'] == 'left':\n",
    "            drop_pose_left = drop_pose_param[i]['position']\n",
    "        else: # right\n",
    "            drop_pose_right = drop_pose_param[i]['position']\n",
    "\n",
    "    # TODO: Rotate PR2 in place to capture side tables for the collision map\n",
    "\n",
    "    # create the dict list\n",
    "    dict_list = []\n",
    "    # Loop through the pick list\n",
    "    for i in range(0, len(object_list_param)):\n",
    "        # Parse parameters into individual variables\n",
    "        object_name.data = object_list_param[i]['name']\n",
    "        object_group = object_list_param[i]['group']\n",
    "\n",
    "        # Assign the arm to be used for pick_place\n",
    "        if object_group == 'green':\n",
    "            arm_name.data = 'right'\n",
    "            object_drop_pos = drop_pose_right\n",
    "        else:  # group == red\n",
    "            arm_name.data = 'left'\n",
    "            object_drop_pos = drop_pose_left\n",
    "\n",
    "        # Get the PointCloud for a given object and obtain it's centroid\n",
    "        # get the centroid from the dictionary\n",
    "        centr = centroids_dic[object_name.data]\n",
    "        pick_pose.position.x = np.asscalar(centr[0])\n",
    "        pick_pose.position.y = np.asscalar(centr[1])\n",
    "        pick_pose.position.z = np.asscalar(centr[2])\n",
    "\n",
    "        # Create 'place_pose' for the object\n",
    "        place_pose.position.x = object_drop_pos[0]\n",
    "        place_pose.position.y = object_drop_pos[1]\n",
    "        place_pose.position.z = object_drop_pos[2]\n",
    "\n",
    "        # Create a list of dictionaries (made with make_yaml_dict()) for later output to yaml format\n",
    "        yaml_dict = make_yaml_dict(test_scene_num, arm_name, object_name, pick_pose, place_pose)\n",
    "        dict_list.append(yaml_dict)\n",
    "        # Wait for 'pick_place_routine' service to come up\n",
    "        #rospy.wait_for_service('pick_place_routine')\n",
    "\n",
    "        #try:\n",
    "        #    pick_place_routine = rospy.ServiceProxy('pick_place_routine', PickPlace)\n",
    "\n",
    "            # TODO: Insert your message variables to be sent as a service request\n",
    "        #    TEST_SCENE_NUM = test_scene_num\n",
    "        #    OBJECT_NAME = object_name\n",
    "        #    WHICH_ARM = arm_name\n",
    "        #    PICK_POSE = pick_pose\n",
    "        #    PLACE_POSE = place_pose\n",
    "\n",
    "        #    resp = pick_place_routine(TEST_SCENE_NUM, OBJECT_NAME, WHICH_ARM, PICK_POSE, PLACE_POSE)\n",
    "\n",
    "        #    print (\"Response: \",resp.success)\n",
    "\n",
    "        #except rospy.ServiceException, e:\n",
    "        #    print \"Service call failed: %s\"%e\n",
    "\n",
    "    # Output your request parameters into output yaml file\n",
    "    print \"now ouput to yaml\"\n",
    "    yaml_filename = 'output_1.yaml' # for test world 1\n",
    "    #yaml_filename = 'output_2.yaml' # for test world 2\n",
    "    #yaml_filename = 'output_3.yaml' # for test world 3\n",
    "    send_to_yaml(yaml_filename, dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "As shown above, I have completely skipped the actual pick and place routine with the lack of time. In the future, I plan to add the routine to the project, and complete the pick and place movement, as well as the fourth world where we were given more freedom in creating the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
